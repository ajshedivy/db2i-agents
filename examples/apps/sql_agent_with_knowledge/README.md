# SQrL: A Text2SQL Reasoning Agent

SQrL is an advanced text-to-SQL system that leverages Reasoning Agents to provide deep insights into data. The system works with IBM Db2 for i sample dataset, which contains enterprise data including employee information, department structure, project management, and business operations.

This demo showcases the power of combining semantic models with agent knowledge to improve SQL query generation and analysis.

## Features

- **Semantic Model Integration**: Uses a pre-defined semantic model to understand table relationships and data structure
- **Dynamic Knowledge Retrieval**: Searches knowledge base for relevant information about tables and columns
- **Advanced Reasoning**: "Thinks" before constructing queries, following a chain-of-thought approach
- **Result Analysis**: Analyzes query results for insights and data quality issues

> Note: Fork and clone the repository if needed

### 1. Install dependencies

Install the dependencies:

- uv:  
    ```bash
    curl -LsSf https://astral.sh/uv/install.sh | sh
    ```

- Install Ollama from [Ollama](https://ollama.com/)
- Pull Ollama models:
    ```bash
    ollama pull qwen3:8b
    # OR try the new recommended model for better performance:
    ollama pull gpt-oss:20b

    # model used for embeddings
    ollama pull openhermes
    ```
    > ðŸ’¡ **Recommended**: Try `gpt-oss:20b` for excellent general performance!

The `uv run` command automatically creates a virtual environment and manages dependencies.

---

### 2. Load the knowledge base

The knowledge base contains table metadata, rules and sample queries, which are used by the Agent to improve responses.

```shell
# Basic usage - loads knowledge from knowledge/sample directory
uv run python load_knowledge.py

# With options to specify directory and recreate knowledge
# uv run python load_knowledge.py --destination knowledge/sample --recreate
```

Available options for load_knowledge.py:
- `--destination` or `-d`: Directory path where description files will be saved (default: knowledge/sample)
- `--recreate`: Overwrite existing files if destination directory already exists
- `--no-load`: Skip loading knowledge into agent (default: False, knowledge is loaded)

### 3. Storage and Persistence

The application uses SQLite for storage, configured in the agent.py file:

```python
# Database connection configuration
db_url = "tmp/agent_data.db"

# SQLite storage for agent sessions
agent_storage = SqliteStorage(
    table_name="sql_agent_sessions",
    db_file=db_url,
    auto_upgrade_schema=True,
)
```

This SQLite database stores conversation history and agent session information.

### 4. Export API Keys

```shell
# Choose one or more of these providers
export ANTHROPIC_API_KEY=***
export OPENAI_API_KEY=***
export GOOGLE_API_KEY=***
export GROQ_API_KEY=***
```

### 5. Run SQL Agent CLI

You can run the SQL Agent directly using the agent.py script. The default model is `ollama:qwen2.5:latest`, but you can specify other models as needed:

```shell
# pull the ollama model
ollama pull qwen2.5
# OR pull the recommended model:
ollama pull gpt-oss:20b

# Basic usage with default model (Ollama qwen2.5)
uv run agent.py

# With recommended gpt-oss model
uv run agent.py --model-id ollama:gpt-oss:20b

# With another model
uv run agent.py --model-id gpt-4o

# With debug mode enabled
uv run agent.py --debug

# With streaming responses
uv run agent.py --stream
```

Available command-line options:
- `--model-id`: Specify which model to use (default: ollama:qwen2.5:latest)
  - Available models: 
    - `o4-mini`
    - `claude-3-7-sonnet`
    - `gpt-4.1`
    - `o3`
    - `gemini-2.5-pro`
    - `gpt-4o`
    - `qwen3:30b-a3b`
    - `qwen3:8b`
    - `qwen2.5`
- `--debug`: Enable debug mode for more verbose logging
- `--stream`: Enable streaming responses to see the agent's thoughts as they're generated

The CLI lets you:
- Ask natural language questions about the IBM Db2i sample dataset
- View SQL queries generated by the agent
- See query results and analysis
- Access conversation history

## Semantic Model

The agent uses a pre-defined semantic model to understand the database structure. This model includes:

```json
{
  "dataset_overview": {
    "organizational_structure": ["DEPARTMENT", "ORG"],
    "personnel_data": ["EMPLOYEE", "STAFF", "EMP_PHOTO", "EMP_RESUME"],
    "project_management": ["PROJECT", "PROJACT", "ACT", "EMPPROJACT"],
    "business_operations": ["SALES", "IN_TRAY", "CL_SCHED"]
  },
  "tables": [
    {
      "table_name": "DEPARTMENT",
      "table_description": "The department table describes each department in the enterprise and identifies its manager and the department that it reports to.",
      "Use Case": "Use this table when analyzing organizational structure, department hierarchies, or when needing information about department managers."
    },
    // Additional tables...
  ]
}
```

## Enhancing the Agent

You can improve the agent's capabilities by:

1. **Enriching the Semantic Model**: Add more table relationships and business context
2. **Adding Table Rules**: Define rules in table metadata to guide query construction
3. **Creating Sample Queries**: Add examples to knowledge/sample_queries.sql to demonstrate complex query patterns

## Example Prompts

Start with simple questions and progress to more complex ones:

- "What tables are available in the database?"
- "Show me the structure of the EMPLOYEE table."
- "Find all employees who work in the RESEARCH department."
- "Which department has the highest average salary?"
- "Show me all projects that are currently active and their managers."

